import streamlit as st
import pandas as pd
import os
from datetime import datetime
from config.page_config import configure_page
from utils.data_loader import load_data
from utils.data_processor import prepare_data_with_real_status
from components.sidebar import create_sidebar_filters
from components.kanban import create_kanban_view
from components.analytics import create_analytics
from components.footer import create_footer

def main():
    """Fun√ß√£o principal da aplica√ß√£o com upload obrigat√≥rio"""
    # Configurar p√°gina
    configure_page()
    
    # T√≠tulo principal
    st.markdown('<h1 class="main-header">üìä An√°lise Semanal dos Chamados</h1>', 
               unsafe_allow_html=True)
    
    # Container principal com margem para o rodap√©
    st.markdown('<div class="main-content">', unsafe_allow_html=True)
    
    # NOVA L√ìGICA: Verificar se os dados j√° est√£o carregados
    if not _check_data_loaded():
        # Se n√£o h√° dados carregados, mostrar interface de upload
        _show_upload_interface()
        return  # Para aqui at√© os dados serem carregados
    
    # Se chegou aqui, os dados est√£o dispon√≠veis
    with st.spinner("‚öôÔ∏è Carregando dados do sistema..."):
        df = load_data()
    
    if df is None:
        st.error("‚ùå Erro ao carregar os dados processados")
        if st.button("üîÑ Recarregar dados"):
            _clear_data_cache()
            st.rerun()
        return
    
    # Preparar dados
    with st.spinner("‚öôÔ∏è Preparando an√°lise com base na Data Alvo..."):
        df = prepare_data_with_real_status(df)
    
    # Verificar se temos dados de DATA_ALVO
    if len(df) == 0:
        st.error("‚ùå Nenhum dado v√°lido encontrado!")
        st.warning("Verifique se a coluna 'DATA_ALVO' existe e cont√©m datas v√°lidas.")
        return
    
    st.success(f"‚úÖ Sistema carregado! {len(df):,} chamados.")
    
    # Criar filtros
    filtros = create_sidebar_filters(df)
    if filtros[0] is None or filtros[1] is None:
        st.warning("‚ö†Ô∏è Dados insuficientes para an√°lise.")
        return
    
    ano, semana, responsavel, status_filtrados = filtros
    
    # Visualiza√ß√µes principais
    create_kanban_view(df, ano, semana, responsavel, status_filtrados)
    create_analytics(df, ano, semana, responsavel, status_filtrados)
    
    # Informa√ß√µes do sistema na sidebar
    _show_system_info(df, ano, semana, responsavel, status_filtrados)
    
    # Bot√£o para recarregar dados na sidebar
    _show_data_management_sidebar()
    
    # Fechar container principal
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Criar rodap√©
    create_footer()

def _check_data_loaded():
    """Verifica se os dados j√° foram carregados e processados"""
    if 'data_processed' in st.session_state and st.session_state.data_processed:
        return True
    
    if os.path.exists("requisicoes_data.parquet"):
        st.session_state.data_processed = True
        return True
    
    return False

def _show_upload_interface():
    """Mostra interface de upload obrigat√≥rio"""
    st.markdown("---")
    st.subheader("üìÅ Upload dos Arquivos Necess√°rios")
    st.info("Para utilizar o sistema, √© necess√°rio fazer upload dos dois arquivos Excel abaixo:")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üìÑ Arquivo 1")
        uploaded_req = st.file_uploader(
            "Relat√≥rio de Requisi√ß√µes.xlsx",
            type=['xlsx'],
            help="Arquivo principal com dados das requisi√ß√µes",
            key="upload_req"
        )
        
        if uploaded_req is not None:
            st.success("‚úÖ Arquivo 1 carregado")
            st.caption(f"Tamanho: {uploaded_req.size / 1024:.1f} KB")
    
    with col2:
        st.markdown("### üìÑ Arquivo 2")
        uploaded_minha = st.file_uploader(
            "Requisi√ß√µes da Minha Equipe.xlsx", 
            type=['xlsx'],
            help="Arquivo complementar da equipe",
            key="upload_minha"
        )
        
        if uploaded_minha is not None:
            st.success("‚úÖ Arquivo 2 carregado")
            st.caption(f"Tamanho: {uploaded_minha.size / 1024:.1f} KB")
    
    # Bot√£o de processamento
    if uploaded_req is not None and uploaded_minha is not None:
        st.markdown("---")
        col_center = st.columns([1, 2, 1])[1]
        
        with col_center:
            if st.button("üöÄ Processar Dados e Iniciar An√°lise", 
                        type="primary", use_container_width=True):
                _process_uploaded_files(uploaded_req, uploaded_minha)
    
    elif uploaded_req is not None or uploaded_minha is not None:
        st.warning("‚ö†Ô∏è Por favor, fa√ßa upload dos dois arquivos para continuar.")
    
    # Informa√ß√µes adicionais
    with st.expander("‚ÑπÔ∏è Informa√ß√µes sobre os arquivos"):
        st.markdown("""
        **Arquivo 1 - Relat√≥rio de Requisi√ß√µes.xlsx:**
        - Cont√©m dados principais dos chamados
        - Deve ter colunas: NUM_CHAMADO, DATA_ABERTURA, STATUS, etc.
        
        **Arquivo 2 - Requisi√ß√µes da Minha Equipe.xlsx:**
        - Cont√©m dados complementares da equipe
        - Deve ter colunas: Requisi√ß√£o de Servi√ßo, Data Esperada, etc.
        
        **Processamento:**
        - Os arquivos s√£o mesclados automaticamente
        - Dados s√£o salvos localmente para pr√≥ximas sess√µes
        - Formato otimizado para melhor performance
        """)

def _process_uploaded_files(uploaded_req, uploaded_minha):
    """Processa os arquivos enviados"""
    try:
        with st.spinner("üîÑ Processando arquivos... Isso pode levar alguns segundos."):
            # Carregar dados dos uploads
            df_req = pd.read_excel(uploaded_req)
            df_req_minha = pd.read_excel(uploaded_minha)
            
            # Aplicar processamento original
            df_final = _process_data_original_logic(df_req, df_req_minha)
            
            # Salvar parquet para pr√≥ximas execu√ß√µes
            df_final.to_parquet("requisicoes_data.parquet", index=False)
            
            # Marcar como processado
            st.session_state.data_processed = True
            
            st.success("‚úÖ Dados processados com sucesso!")
            st.balloons()
            
            # Recarregar a p√°gina para mostrar o dashboard
            st.rerun()
            
    except Exception as e:
        st.error(f"‚ùå Erro ao processar arquivos: {str(e)}")
        st.error("Verifique se os arquivos est√£o no formato correto e tente novamente.")
        import traceback
        with st.expander("Detalhes do erro (para debug)"):
            st.code(traceback.format_exc())

def _process_data_original_logic(df_req, df_req_minha):
    """Aplica a l√≥gica original de processamento dos dados"""
    # Verificar se a coluna RESOLVEDOR_PADRAO existe
    if 'RESOLVEDOR_PADRAO' in df_req.columns:
        df_req = df_req[df_req['RESOLVEDOR_PADRAO'] == 'AUTOMA√á√ÉO TELECOM'].copy()
    
    # Selecionar apenas colunas que existem no df_req
    required_cols_req = ['NUM_CHAMADO', 'DATA_ABERTURA', 'DATA_PREV_SOLUCAO',
                         'DATA_QUEBRA_SLA', 'SLA_VIOLADO', 'DATA_RESOLUCAO',
                         'DATA_FECHAMENTO', 'Status', 'TITULO', 'SOLICITANTE', 'RESPONSAVEL',
                         'EMPRESA_SOLICITANTE', 'CLIENTE_CIDADE', 'CLIENTE_UF']
    
    available_cols_req = [col for col in required_cols_req if col in df_req.columns]
    df_req = df_req[available_cols_req].copy()
    
    # Selecionar apenas colunas que existem no df_req_minha
    required_cols_minha = ['Requisi√ß√£o de Servi√ßo','Data Esperada', 'Resumo', 
                           'Status', 'Propriet√°rio', 'Cliente', 'Criado em', 
                           'Resolvido em', 'SLA - Data Prevista Solu√ß√£o', 'SLA - Data Quebra']
    
    available_cols_minha = [col for col in required_cols_minha if col in df_req_minha.columns]
    df_req_minha = df_req_minha[available_cols_minha].copy()
    
    # Renomear colunas apenas se existem
    rename_dict = {}
    if 'Requisi√ß√£o de Servi√ßo' in df_req_minha.columns:
        rename_dict['Requisi√ß√£o de Servi√ßo'] = 'NUM_CHAMADO'
    if 'Resumo' in df_req_minha.columns:
        rename_dict['Resumo'] = 'TITULO'
    if 'Propriet√°rio' in df_req_minha.columns:
        rename_dict['Propriet√°rio'] = 'RESPONSAVEL'
    if 'Cliente' in df_req_minha.columns:
        rename_dict['Cliente'] = 'SOLICITANTE'
    if 'Resolvido em' in df_req_minha.columns:
        rename_dict['Resolvido em'] = 'DATA_RESOLUCAO'
    if 'Criado em' in df_req_minha.columns:
        rename_dict['Criado em'] = 'DATA_ABERTURA'
    if 'SLA - Data Prevista Solu√ß√£o' in df_req_minha.columns:
        rename_dict['SLA - Data Prevista Solu√ß√£o'] = 'DATA_PREV_SOLUCAO'
    if 'SLA - Data Quebra' in df_req_minha.columns:
        rename_dict['SLA - Data Quebra'] = 'DATA_QUEBRA_SLA'
    # ‚ö†Ô∏è IMPORTANTE: Renomear 'Status' para 'STATUS_MINHA' para evitar duplicatas
    if 'Status' in df_req_minha.columns:
        rename_dict['Status'] = 'STATUS_MINHA'
    
    df_req_minha.rename(columns=rename_dict, inplace=True)
    
    # Identificar colunas novas 
    colunas_df_req = set(df_req.columns)
    colunas_df_req_minha = set(df_req_minha.columns)
    colunas_novas = colunas_df_req_minha - colunas_df_req
    
    # Fazer o merge apenas se NUM_CHAMADO existe em ambos
    if 'NUM_CHAMADO' in df_req.columns and 'NUM_CHAMADO' in df_req_minha.columns:
        colunas_para_merge = ['NUM_CHAMADO'] + list(colunas_novas)
        df_req_minha_filtrado = df_req_minha[colunas_para_merge].copy()
        df_final = pd.merge(df_req, df_req_minha_filtrado, on='NUM_CHAMADO', how='left')
    else:
        # Se n√£o conseguir fazer merge, usar apenas df_req
        df_final = df_req.copy()
    
    # Tratar colunas nulas
    if 'RESPONSAVEL' in df_final.columns:
        df_final['RESPONSAVEL'] = df_final['RESPONSAVEL'].fillna('Propriet√°rio Vazio')
    
    # Criar DATA_ALVO com nova l√≥gica: prioridade √© Data Esperada > DATA_QUEBRA_SLA > DATA_PREV_SOLUCAO
    from datetime import datetime
    hoje = datetime.now().date()
    
    def get_data_alvo(row):
        # 1. Tentar usar Data Esperada (do arquivo 2 - Requisi√ß√µes da Minha Equipe)
        data_esp = row.get('Data Esperada')
        if pd.notna(data_esp):
            data_esp_date = pd.to_datetime(data_esp, errors='coerce')
            if pd.notna(data_esp_date):
                return data_esp_date
        
        # 2. Se Data Esperada n√£o existe ou est√° vazia, usar DATA_QUEBRA_SLA
        data_quebra = row.get('DATA_QUEBRA_SLA')
        if pd.notna(data_quebra):
            return data_quebra
        
        # 3. Se DATA_QUEBRA_SLA n√£o existe, usar DATA_PREV_SOLUCAO
        data_prev = row.get('DATA_PREV_SOLUCAO')
        if pd.notna(data_prev):
            return data_prev
        
        return None
    
    # Converter Data Esperada se existir
    if 'Data Esperada' in df_final.columns:
        df_final['Data Esperada'] = pd.to_datetime(df_final['Data Esperada'], errors='coerce')
    
    df_final['DATA_ALVO'] = df_final.apply(get_data_alvo, axis=1)
    
    # Aplicar padroniza√ß√£o de colunas
    df_final = _apply_column_mapping(df_final)
    
    return df_final

def _apply_column_mapping(df):
    """Aplica mapeamento de colunas para padronizar nomes"""
    column_mapping = {
        'NUM_CHAMADO': 'REQUISICAO',
        'Status': 'STATUS',
        'TITULO': 'RESUMO',
    }
    
    # Aplicar mapeamento apenas para colunas existentes
    existing_columns = {k: v for k, v in column_mapping.items() if k in df.columns}
    df = df.rename(columns=existing_columns)
    return df

def _show_data_management_sidebar():
    """Mostra op√ß√µes de gerenciamento de dados na sidebar"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("üîß Gerenciar Dados")
    
    if st.sidebar.button("üóëÔ∏è Limpar dados e recarregar"):
        _clear_data_cache()
        st.rerun()
    
    st.sidebar.caption("Use para carregar novos arquivos")

def _clear_data_cache():
    """Limpa cache de dados"""
    if 'data_processed' in st.session_state:
        del st.session_state.data_processed
    
    # Remover arquivo parquet se existir
    if os.path.exists("requisicoes_data.parquet"):
        os.remove("requisicoes_data.parquet")
    
    # Limpar cache do streamlit
    st.cache_data.clear()

def _show_system_info(df, ano, semana, responsavel, status_filtrados):
    """Mostra informa√ß√µes do sistema na sidebar"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("‚ÑπÔ∏è Informa√ß√µes do Sistema")
    st.sidebar.caption(f"Data atual: {datetime.now().strftime('%d/%m/%Y')}")
    st.sidebar.caption(f"√öltima atualiza√ß√£o: {datetime.now().strftime('%H:%M:%S')}")
    
    # Intervalo de datas dos dados
    if 'DATA_ALVO' in df.columns and len(df) > 0:
        data_min = df['DATA_ALVO'].min().strftime('%d/%m/%Y')
        data_max = df['DATA_ALVO'].max().strftime('%d/%m/%Y')
        st.sidebar.caption(f"Per√≠odo: {data_min} - {data_max}")
    
    # Estat√≠sticas dos filtros
    df_total = df[
        (df['ANO_ALVO'] == ano) & 
        (df['SEMANA_ALVO'] == semana)
    ]
    
    if responsavel != 'Todos':
        df_total = df_total[df_total['RESPONSAVEL'] == responsavel]
    
    if status_filtrados != 'Todos':
        df_filtrado = df_total[df_total['STATUS'].isin(status_filtrados)]
        st.sidebar.caption(f"Registros filtrados: {len(df_filtrado):,} de {len(df_total):,}")
        if len(df_total) > 0:
            percentual = (len(df_filtrado) / len(df_total)) * 100
            st.sidebar.caption(f"Percentual exibido: {percentual:.1f}%")
    else:
        st.sidebar.caption(f"Total de registros: {len(df_total):,}")


if __name__ == "__main__":
    main()